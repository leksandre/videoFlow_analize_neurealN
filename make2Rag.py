import os
import requests
from bs4 import BeautifulSoup
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document

# ========== –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ==========
OUTPUT_DIR = "domotel_2var_documents"

URLS = [
    "https://xn--d1acscjb2a6f.xn--p1ai/", 
    "https://xn--80az8a.xn--d1aqf.xn--p1ai/%D1%81%D0%B5%D1%80%D0%B2%D0%B8%D1%81%D1%8B/%D0%BA%D0%B0%D1%82%D0%B0%D0%BB%D0%BE%D0%B3-%D0%BD%D0%BE%D0%B2%D0%BE%D1%81%D1%82%D1%80%D0%BE%D0%B5%D0%BA/%D0%BE%D0%B1%D1%8A%D0%B5%D0%BA%D1%82/58154", 
    "https://xn--80az8a.xn--d1aqf.xn--p1ai/%D1%81%D0%B5%D1%80%D0%B2%D0%B8%D1%81%D1%8B/%D0%B5%D0%B4%D0%B8%D0%BD%D1%8B%D0%B9-%D1%80%D0%B5%D0%B5%D1%81%D1%82%D1%80-%D0%B7%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B9%D1%89%D0%B8%D0%BA%D0%BE%D0%B2/%D0%B7%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B9%D1%89%D0%B8%D0%BA/17697" 
]

# ========== –§—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∞–π—Ç–∞ ==========
def parse_website(url):
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')

        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for script in soup(["script", "style"]):
            script.extract()

        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
        lines = (line.strip() for line in soup.get_text().splitlines())
        text = '\n'.join(line for line in lines if line)

        return text[:5000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
    except Exception as e:
        print(f"[–û—à–∏–±–∫–∞] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å {url}: {e}")
        return ""

# ========== –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π ==========
def create_knowledge_base(urls):
    docs = []

    print("üîÑ –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤...")
    for url in urls:
        print(f"üåê –ü–∞—Ä—Å–∏–Ω–≥: {url}")
        content = parse_website(url)
        if content:
            docs.append({
                "text": content,
                "metadata": {"source": url}
            })

    print(f"üì• –°–ø–∞—Ä—Å–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docs)}")

    print("üß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    embeddings = HuggingFaceEmbeddings(model_name="all-mpnet-base-v2")#–ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è, –Ω–æ —á—É—Ç—å –º–µ–¥–ª–µ–Ω–Ω–µ–µ
    #embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")#–ë—ã—Å—Ç—Ä–∞—è –∏ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è, –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á
    #embeddings = HuggingFaceEmbeddings(model_name="multi-qa-mpnet-base-dot-v1")#–•–æ—Ä–æ—à–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º

    print("üìö –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
    documents = [Document(page_content=d["text"], metadata=d["metadata"]) for d in docs]
    vectorstore = Chroma.from_documents(documents, embeddings, persist_directory=OUTPUT_DIR)
    vectorstore.persist()

    print(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ø–∞–ø–∫—É: '{OUTPUT_DIR}'")

# ========== –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ ==========
if __name__ == "__main__":
    if os.path.exists(OUTPUT_DIR):
        print(f"‚ö†Ô∏è –ü–∞–ø–∫–∞ '{OUTPUT_DIR}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –£–¥–∞–ª–∏—Ç–µ –µ—ë, –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å.")
    else:
        create_knowledge_base(URLS)